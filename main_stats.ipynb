{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1><center> Non-parametric comparison of convolutional neural networks and CaiT transformer in the classification of COVID-19 in chest CT scans </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Table of Contents</h2>\n",
    "    <ul>\n",
    "    <li><a href=\"#Section_1\">Import libraries and supporting functions </a></li>\n",
    "    <li><a href=\"#Section_2\"> Build the main comparison function</a> </li>\n",
    "    <li><a href=\"#Section_3\">Define parameters and run the comparison</a></li>\n",
    "    <li><a href=\"#Section_4\"> Extract the result in LaTeX tables </a></li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2 id=\"#Section_1\">Import libraries and supporting functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Here we first import the libraries and functions required to run the comparison__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SC5Bv3aLUI4m",
    "outputId": "b3499e22-e9e3-4792-ded3-bfbc06e5e5a0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from stats.non_parametric_stats import *\n",
    "from stats.utils import df_to_latex\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "sns.set(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2 id=\"Section_2\"> Build the main comparison function </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we extract information from the row data and start the comparison process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main(**params):\n",
    "    \"\"\"\n",
    "    :param params: a dictionary to access the data runs, group the data by\n",
    "    experiment or by network architecture, num of bootstrap samples, plot and\n",
    "    save plots and dataframes in csv format. Besides, it provides a summary\n",
    "    of the accuracies at training, validation and test. It also summarizes of\n",
    "    all performance metrics, before and after bootstrapping. The posthoc\n",
    "    friedman-nemenyi test stats also are in the dictionary for all performance\n",
    "    metrics at test(accuracy, balanced accuracy, F1, F2, MCC, sensibility, specificity).\n",
    "    :return: a dictionary with the summary of the accuracy during train, validation and test\n",
    "    results, evaluation metrics and probabilities.\n",
    "    \"\"\"\n",
    "    # Raise an assertion if incorrect grouping\n",
    "    assert params['group'] in ['Architecture', 'Experiment'], 'Group by Architecture or Experiment'\n",
    "\n",
    "    # Save output in a dictionary\n",
    "    outputs = {}\n",
    "\n",
    "    # Read data\n",
    "    df = pd.read_csv(os.path.join(params['root'], params['data']))\n",
    "\n",
    "    # Sorting by run architecture, loss and optimizer\n",
    "    df = df.sort_values(['Run', 'Architecture', 'Loss', 'Optimizer'])\n",
    "\n",
    "    # Abbreviate architecture name for plotting purpose\n",
    "    df = df.replace({'MobileNet-v3-large': 'MobileNet-v3'})\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'Exp': 'Experiment', 'Sensitivity': 'Sens',\n",
    "                            'F1 macro': 'F1', 'Specificity': 'Spec', 'Max epoch': 'Epoch'})\n",
    "    # Re-number experiment as Exp-xx\n",
    "    df['Experiment'] = ['Exp-' + str(x).zfill(2) for x in df['Experiment']]\n",
    "\n",
    "    #  Create the experimental set-up\n",
    "    df_set = df[['Experiment', 'Architecture', 'Loss', 'Optimizer']][0:20].set_index('Experiment')\n",
    "    df_set = df_set.sort_values(by='Experiment')\n",
    "    outputs['exp_setup'] = df_set\n",
    "\n",
    "    # Filter data by training, validation and test accuracies\n",
    "    df_acc = df.rename(columns={'Accuracy': 'Test acc', 'val_acc': 'Val acc',\n",
    "                                'tr_acc': 'Train acc'})\n",
    "\n",
    "    # Create directories to save figures and csv files\n",
    "    outdir = os.path.join(params['root'], 'figures', params['group'].lower())\n",
    "    csv_dir = os.path.join(params['root'], 'csv_files', params['group'].lower())\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    dirs = [outdir, csv_dir]\n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "            print('Directory created')\n",
    "\n",
    "    # Dataframe summarising train,test and validation accuracies\n",
    "    acc_sum = acc_summary(params['group'], params['acc'], df_acc, outdir, csv_dir, \n",
    "                          params['boxplot'], params['hideplot'])\n",
    "    # Save results in a dictionary\n",
    "    outputs['accuracies'] = acc_sum\n",
    "\n",
    "    # Filtering to get data only for the metrics to be compared\n",
    "    df_covid = df.filter(['Experiment', 'Architecture', 'Loss', 'Optimizer', 'Accuracy', 'BA',\n",
    "                          'MCC', 'F1', 'F2', 'Sens', 'Spec', 'Max acc', 'Epoch'])\n",
    "    df_covid = df_covid.rename(columns={'Accuracy': 'Acc'})\n",
    "\n",
    "    # Provide a summary of all evaluation metrics before bootstrapping\n",
    "    if params['stats_sum']:\n",
    "        st_s = stats_summary(params['group'], params['metrics'], df_covid, csv_dir)\n",
    "        outputs['pre_boots'] = st_s\n",
    "\n",
    "    # Bootstrapping summary and post-hoc test\n",
    "    ranks, intervals, post_hoc = bootstrap_stats_summary(params['group'], params['metrics'], \n",
    "                                                         df_covid, params['n_bootstraps'], \n",
    "                                                         outdir, csv_dir, params['alpha'], \n",
    "                                                         params['hideplot'],  params['nemenyi'])\n",
    "    # Save into the dictionary\n",
    "    outputs['ranks'], outputs['ci'] = ranks, intervals\n",
    "    outputs['stats_comp'] = post_hoc\n",
    "\n",
    "    # Bootstrap for maximum acc and epoch - training\n",
    "    epochs, max_acc = bootstrapping_epochs(params['n_bootstraps'], df_covid,\n",
    "                                           'Epoch', 'Max acc', params['group'])\n",
    "\n",
    "    # boxplot for bootstrapped epoch with max accuracy and accuracy during validation\n",
    "    mx_rank, ci_pval, epochs_acc = boots_epochs_df(epochs, max_acc, params['group'], outdir, \n",
    "                                                   csv_dir, params['boxplot'], \n",
    "                                                   params['hideplot'], nemenyi=params['nemenyi'])\n",
    "\n",
    "    # Save into the dictionary max accuracy ranking, pvalues and confidence intervals\n",
    "    outputs['max_rank'], outputs['max_acc_stats'] = mx_rank, ci_pval\n",
    "\n",
    "    # density distribution for bootstrapped max accuracy and number of epochs during validation\n",
    "    if params['dist_plot']:\n",
    "        for a in ['epochs', 'max acc']:\n",
    "            max_acc_epoch_plot(epochs_acc, params['group'], col_name=a, outdir=outdir,\n",
    "                               hideplot=params['hideplot'])\n",
    "\n",
    "    # Extract the friedman statistic and associated p-value\n",
    "    friedman_test = {k: v[0:2] for (k, v) in post_hoc.items()}\n",
    "\n",
    "    # Update dictionary to include Friedman-Nemenyi stats for the maximum accuracy and the number\n",
    "    # of training epochs\n",
    "    friedman_test.update({k: v for k, v in ci_pval.items() if k == 'Max accuracy' or\n",
    "                          k == 'Epochs'})\n",
    "    pval = pd.DataFrame.from_dict(friedman_test).T.reset_index()\n",
    "    pval.columns = ['Metric', 'Friedman', 'p-value']\n",
    "\n",
    "    # Put together all post-hoc stats and p-value into the dictionary\n",
    "    outputs['pval'] = pval\n",
    "\n",
    "    # save dataframe\n",
    "    pval.to_csv(os.path.join(csv_dir, params['group'].lower() + '_pval.csv'))\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2 id=\"Section_3\">Define parameters and run the comparison </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    acc=['Train acc','Val acc','Test acc']\n",
    "    metrics = ['Acc','BA','F1', 'F2','MCC','Sens','Spec']\n",
    "    \n",
    "    args = {'root': '/Users/aze_ace/Documents/pythonProject/covid_project', 'data':'covid_10_runs'\n",
    "                                                                                   '.csv',\n",
    "        'acc':acc, 'stats_sum': True, 'group': 'Architecture', 'metrics': metrics,'n_bootstraps': 1000,\n",
    "        'alpha': 5.0, 'nemenyi': True, 'hideplot': True, 'boxplot': True, 'dist_plot': True}\n",
    "    \n",
    "    sum_net= main(args)\n",
    "    \n",
    "    args['group'] = 'Experiment'\n",
    "    \n",
    "    sum_exp = main(args)\n",
    "    \n",
    "    prob_bar(os.path.join(args['root'], 'figures'))\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2 id=\"Section_3\"> Extract the result in LaTeX tables  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define path to save the tables and create \n",
    "\n",
    "path = '/Users/aze_ace/Documents/pythonProject/covid_project/tables'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Experimental setup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_setup='ex_setup'\n",
    "caption= 'Experimetal design'\n",
    "label= 'tab: exp_setup'\n",
    "df1= sum_exp['exp_setup']\n",
    "df_to_latex(f_setup, caption, label, df1, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Results tables by experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_acc='ex_acc1'\n",
    "caption= 'Train, validation and test accuracies by experiment.'\n",
    "label= 'tab: exp_acc'\n",
    "df2= sum_exp['accuracies']\n",
    "df_to_latex(f_acc, caption, label, df2, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_nacc='ex_metrics'\n",
    "caption= 'Evaluation metrics by experiment before bootstrapping.'\n",
    "label= 'tab: exp_metrics'\n",
    "df3= sum_exp['pre_boots']\n",
    "df_to_latex(f_nacc , caption, label, df3, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_rank='ex_rank'\n",
    "caption= 'Ranks and median by experiment after bootstrapping.'\n",
    "label= 'tab: exp_metrics'\n",
    "df4= sum_exp['ranks']\n",
    "df_to_latex(f_rank , caption, label, df4, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_ci = 'ex_ci'\n",
    "caption= 'Confidence interval by experiment after bootstrapping.'\n",
    "label= 'tab: expci'\n",
    "df5= sum_exp['ci']\n",
    "df_to_latex(f_ci, caption, label, df5, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_maxacc='ex_epoch'\n",
    "caption= 'Ranking of the maximum training accuracy and training epochs with medians by experiment'\n",
    "label= 'tab: exp_epoch'\n",
    "df6= sum_exp['max_rank']\n",
    "df_to_latex(f_maxacc, caption, label, df6, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### c)  Results tables by architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_acc='net_acc'\n",
    "caption= 'Train, validation and test accuracies by architecture.'\n",
    "label= 'tab: net_acc'\n",
    "df7= sum_net['accuracies']\n",
    "df_to_latex(f_acc, caption, label, df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_nacc='net_metrics'\n",
    "caption= 'Evaluation metrics by architecture before bootstrapping.'\n",
    "label= 'tab: net_metrics'\n",
    "df8= sum_net['pre_boots']\n",
    "df_to_latex(f_nacc , caption, label, df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_rank='net_rank'\n",
    "caption= 'Ranks and median by architecture after bootstrapping.'\n",
    "label= 'tab: net_metrics'\n",
    "df9= sum_net['ranks']\n",
    "df_to_latex(f_rank , caption, label, df9, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_ci = 'net_ci'\n",
    "caption= 'Confidence interval by architecture after bootstrapping.'\n",
    "label= 'tab: net_ci'\n",
    "df10= sum_net['ci']\n",
    "df_to_latex(f_ci, caption, label, df10, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_maxacc='net_epoch'\n",
    "caption= 'Ranking of the maximum training accuracy and training epochs with medians by architecture.'\n",
    "label= 'tab: net_epoch'\n",
    "df11= sum_net['max_rank']\n",
    "df_to_latex(f_maxacc, caption, label, df11, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### d) Friedman statistic and p-value table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "df_pval=sum_net['pval']\n",
    "df_pval_exp=sum_exp['pval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pval_net_exp= pd.concat([df_pval, df_pval_exp.iloc[:, 1:]], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "headers =  ['Metric','Friedman-Arch', 'pval-Arch', 'Friedman-Exp', 'pval-Exp']\n",
    "pval_net_exp.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df12 = pval_net_exp.set_index('Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_nacc='Friedman'\n",
    "caption= 'Friedman statistic and p-values by architectures and experiments at a confidence level alpha =0.05.'\n",
    "label= 'tab: p-val'\n",
    "df_to_latex(f_nacc , caption, label, df12, path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "read_weigths.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
